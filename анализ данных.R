# Урок 1. Начало работы в R

# Основные объекты R
x <- 10 # Создаем переменную X
x
x.1 <- 17
x.1
rm(x.1) # удаляем обьект x.1
x.1
# Основные математические функции «+» , «-», «:», « * », степень «^»,
# квадратный корень sqrt(), log2(), factorial(), exp()
sqrt(25)
log2(256)
log10(1000)
exp(1)
log(exp(1))
factorial(5)
# С помощью функции help() или «?» можно вызвать справку
help(log)
# Множества и действия с ними
a <- c(0, 1, 2, 3)
a[2]
b <- c(rep(2, time=3))
b
b.1 <- rep(2, 4)
b.1
rep(c(0, 2), time=2)
rep(c(0, 2), each=2)
a + b.1
seq(2, 8, by=2)  # четные
seq(1, 9, by=2)  # нечетные
seq(1, 10, length.out=5)
rnorm(n=50, mean=0, sd=1)  # задаем нормальное распределение
rpois(100, 10)  # распределение Пуассона
rbinom(100, 10, 0.5)  # биномиальное распределение
# Текстовый вектор
letters
LETTERS
paste(letters, set="_", seq(1, 26))
?ISOdate # справка
format(ISOdate(2019, 9, 1:30), "%d")
format(ISOdate(2019, 9, 1:30), "%b")
paste0(format(ISOdate(2019, 9, 1:30), "%d"), set="_", rep("sep", 30))
# Прочие основные и востребованные функции
a <- c(rep(1, 10))
a
class(a)
class(letters)
class(2 != 0)
class(factorial)
d <- c(rep("a", 4), rep("b", 6), rep("c", 2))
sample(d)
d.table <- table(d)
d.table
r <- seq(1, 10)
r
sum(r)
r_mean <- sum(r) / length(r)
r_mean
mean(r)
# Функции set.seed(), sort()
set.seed(42)
popul <- rnorm(100)
popul
popul <- round(popul, 3)
popul
sort(popul)
sort(popul, decreasing=TRUE)
# sample(), data.frame(), head(), order(), $
set.seed(42) # используется для установки начального значения для генератора случайных чисел.
lets <- sample(letters, 100, replace=TRUE)
lets
df <- data.frame(lets, popul)
head(df, 10)
# отсортируем строки датафрейма по возрастанию чисел
ind <- order(df$popul)
ind
# есть DataFrame (предположим, назовем его df), и ind - это индексы строк, которые вы хотите выбрать. Код создает новый DataFrame df_new, содержащий только те строки, которые соответствуют индексам в ind.
df_new <- df[ind,]
head(df_new, 10)
# Что еще можно делать с векторами?
g <- seq(31, 45)
g
length(g)
g[1]
g[c(1, 5, 15)]
g[-c(1, 5, 15)]
g[1:5]
h <- 1:15
g + h # переменная g + h будет содержать вектор, в котором каждый элемент представляет собой сумму соответствующих элементов векторов g и h.
g * 2 # Операция умножения вектора на число в R также выполняется поэлементно.
# Логические функции и операторы
5 > 6
61 < 100
2 != 3
2 == 2
6 > 7 | 8 > 9
6 > 7 & 8 > 9
((TRUE == FALSE) & (1 == 1)) & 100 == 100
((TRUE == FALSE) & (1 == 1)) | 100 == 100
# Построим свой dataframe
weight <- c(78, 56, 67, 48, 69, 90)
height <- c(170, 160, 165, 159, 170, 185)
sex <- (c(rep("F",3),rep("M" ,3)))
sex
df_1 <- data.frame(weight, height, sex)
str(df_1)
df_2 <- data.frame(weight, height, sex, stringsAsFactors=TRUE)
str(df_2)
# Стоим матрицу
m <- 1:30
m
dim(m) <- c(10, 3)
m
class(m)
# другой способ
y <- 1:50
m <- matrix(y, 10, 5)
m
rownames(m) # Здесь создается матрица 10x5 с использованием вектора y, и выводится сама матрица и имена строк (в данном случае, строки просто нумеруются от 1 до 10).
# Когда вы создаете матрицу с использованием функции matrix без явного указания имен строк, имена строк по умолчанию устанавливаются в NULL.
colnames(m)
# тоже и для имён столбцов.
names <- LETTERS[1:10]
rownames(m) <- names
colnames(m) <- paste("day", 1:5)
m
# Датасеты
data()  # Информация о встроенных датасетах
# Загрузим датасет с ирисами
head(iris, 10)
sum(is.na(iris)) # используется для подсчета общего количества пропущенных значений во всем датафрейме iris
head(is.na(iris), 10)
dim(iris) #  используется для получения размера (количество строк и столбцов) объекта данных в R
str(iris) # Если iris - это встроенный датасет в R, то str(iris) покажет общую структуру данных и тип каждой переменной.
unique(iris[,5]) # покажет уникальные значения в переменной Species, которые являются уровнями фактора. у нас есть три уникальных вида ирисов: "setosa", "versicolor", и "virginica".
levels(iris$Species)
head(iris[iris$Sepal.Width>3.0,]) # Не забудьте про "," в конце!
head(iris[iris$Sepal.Width > 3.0 & iris$Petal.Width > 1.4,])

percentage <- prop.table(table(iris$Species)) * 100
cbind(freq=table(iris$Species), percentage=percentage)
summary(iris)

# Разделим датасет на вектор x и целевую переменную y
x <- iris[,1:4]
y <- iris[,5]
# Функция boxplot создаст четыре ящичные диаграммы (по одной для каждого числового столбца) с подписями, указывающими название соответствующего столбца в iris.
# Функция par(mfrow = c(1, 4)) используется для того, чтобы ящичные диаграммы были расположены в одну строку и четыре столбца.
par(mfrow=c(1,4))
for(i in 1:4) {
  boxplot(x[,i], main=names(iris)[i])
}
plot(y)

# загрузит и установит пакет caret из репозитория CRAN. После установки пакета, вы сможете успешно выполнить команду library("caret").
# Установка пакета caret, если его нет
install.packages("caret")
install.packages("ellipse")
# Загрузка пакета caret
library("caret")
# dev.off() # Закрытие графических устройств при проблемме
featurePlot(x=x, y=y, plot="ellipse")


# Обучим модельку
install.packages("randomForest")
data_split <- createDataPartition(iris$Species, p=0.8, list=FALSE)

test_data <- iris[-data_split,]  # Оставляем 20% датасета для тестирования
train_data <- iris[data_split,]  # На остальных 80% будем обусать модель

control <- trainControl(method="cv", number=10)
metric <- "Accuracy"

set.seed(7)
fit.rf <- train(Species~., data=train_data, method="rf", metric=metric, trControl=control)

fit.rf$resample

predictions <- predict(fit.rf, test_data)
confusionMatrix(predictions, test_data$Species)

## Приобретенные навыки
'
1. Познакомились со средой R и ее объектами
2. Узнали множество функций для работы с данными
3. Научились задавать последовательности и множества
4. Научились извлекать необходимую информацию из готовых фреймов и выбирать поднаборы данных
5. Научились строить свои набор данных и матрицу
'
#END

##### Урок 2. Обработка данных для анализа
'Одним из важнейших этапов при работе с данными является их подготовка для дальнейшего статистического анализа'

#Условие if-else
library(cluster)
head(votes.repub)
colMeans(votes.repub)

if (mean(votes.repub[,30], na.rm = TRUE) > 60) {
  print("Республиканцы набрали высокий процент голосов")
} else {
  print("Республиканцы набрали < 60% голосов")
}

if (mean(votes.repub[,1], na.rm = TRUE) > 60) {
  print("Республиканцы набрали высокий процент голосов")
} else {
  print("Республиканцы набрали < 60% голосов")
}

# Также есть похожая функция ifelse()
x <- c(1, 0, 0, 0, 1, 0)
ifelse(x != 0, "Yes", "No")
ifelse(
  colMeans(votes.repub, na.rm = TRUE) > 60,
  "Республиканцы набрали высокий процент голосов",
  "Республиканцы набрали < 60% голосов"
)

## Пишем собственную функцию
'Для написания функции воспользуемся набором, содержащим данные о продажах мячей в первые дни 2018 года
* date – год-месяц-день
* ball – теннисные мячи продавались по одному, а также распакованными по 3 и 5 мячей
* price – цены'

date_list <- c(rep("2018-01-01", 4), rep("2018-01-05", 6))
ball_list <- c(rep(1, 3), rep(3, 4), rep(5, 3))
price_list <- c(rep(90, 3), rep(150, 4), rep(390, 3))

df <- data.frame("date"=date_list, "ball"=ball_list, "price"=price_list)
head(df, 10)
# Сохраним датасет в файл
write.csv(df, "df.csv", row.names=FALSE)
# Загрузим датасет из файла
df <- read.csv("df.csv", header=TRUE, sep=",")
head(df, 10)

str(df)
unique(df$ball)
unique(df$price)

# Для работы с датами загружаем пакет "lubridate"
library(lubridate)
class(df$date)
dayn = ymd(df$date)
dayn
# Напишем функцию, которая считает, на какую сумму продано мячей за определенный день
'Поскольку дни в месяцах совпадают, нам нужна функция с аргументами, например, месяц и день'
sum_balls <- function(dataset, m, d) {
  sum(dataset$price[month(dataset$date) == m & day(dataset$date) == d])
}
sum_balls(df, 1, 1)
# Используем цикл for, чтобы вывести сумму за определенный день
for (i in 1:5) {
  print(sum_balls(df, 1, i))
}
# Функции lapply() и sapply()
head(lapply(votes.repub, sum))
sapply(votes.repub, sum)

# tapply()
"В статистике часто нужно разделять данные на группы и применять к ним определенную функцию.
Посмотрим, как работает эта функция на примере набора «Orange».
Набор содержит три переменные:
-номер дерева;
-возраст дерева с 31.12.1968, в днях;
-длина окружности ствола.
Обратим внимание, что измерения ствола дерева производились в одном и том же возрасте. Предположим, мы хотим узнать, на сколько миллиметров изменился диаметр ствола с первой даты измерения, которая соответствует возрасту 118 дням до последней даты, соответствующей 1582 дням.
Напишем функцию, которая эта делала бы, а затем применим ее к каждой подгруппе, определенной фактором Tree с помощью функции tapply()
Предположим, мы хотим узнать, на сколько миллиметров изменился диаметр ствола с первой даты измерения, которая соответствует возрасту 118 дням до последней даты, соответствующей 1582 дням (используем датасет Orange)"
l <- function(x) {
  diff(range(x))
}
tapply(Orange$circumference, Orange$Tree, l)

# Что делать, если у нас есть функции с одинаковыми названиями в разных пакетах?
"(они могут выполнять разные действия)
При вызове функции, R сначала производит поиск в global environment. И если находится функция, то первая выбирается она.
Или R проводит search path и выбирает ту функцию, которую встречает первую на пути"
search()
# С помощью :: показываем R, из какого пакета нам нужна функция
lubridate::dmy("01/01/1970")

#Метод Монте-Карло
"Существует много способов вычисления числа Пи.
Самым простым и понятным является численный метод Монте-Карло, суть которого сводится к простейшему перебору точек на площади.
Суть расчета заключается в том, что мы берем квадрат со стороной $a = 2*R$, вписываем в него круг радиусом $R$.
И начинаем наугад ставить точки внутри квадрата.
Геометрически, вероятность $P1$ того, чтот точка попадет в круг, равна отношению площадей круга и квадрата:
$P1=S_{круг} / S_{квадрат} = πR^2 / a^2 = πR^2 / (2R)^2= π^2 / (2)^2 = π / 4$
Выглядит это так:
"
runs <- 100000
# runif samples from a uniform distribution
xs <- runif(runs, min=-0.5, max=0.5)
ys <- runif(runs, min=-0.5, max=0.5)
in.circle <- xs^2 + ys^2 <= 0.5^2
mc.pi <- (sum(in.circle)/runs)*4
plot(
  xs, ys, pch='.',
  col=ifelse(in.circle, "blue", "grey"),
  xlab='', ylab='', asp=1,
  main=paste("MC Approximation of Pi =", mc.pi)
)
# Пакет "dplyr" и оператор pipe
library(dplyr)
iris %>% dim
head(iris %>% filter(Species=="versicolor"))
head(iris %>% filter(Species=="versicolor") %>% select(Petal.Length))
vc <- as.numeric(iris %>% filter(Species=="versicolor") %>% select(Petal.Length) %>% unlist)
vc
'делаем то же самое, найдем длину лепестков , только для "virginica".
И с помощью оператора %in% посмотрим, сколько совпадений в длине лепестков двух разных видов цветка'
vg <- as.numeric(iris %>% filter (Species=="virginica") %>% select(Petal.Length) %>% unlist)
vg
vc %in% vg
sum(vc %in% vg)
# Сохраняем набор данных в формате CSV
a <- c(1, 2, 3)
b <- c(0, 0, 0)
nm <- c("1_row", "2_row", "3_row")
df <- data.frame(a, b)
df
df["rownames"] = nm
df
df <- df[c("rownames", "a", "b")]
df
filename <- "new_dataframe.csv"
write.csv(df, file=filename)
# Функция read.csv() содержит множество аргументов
"Рассмотрим некоторые из них"
read.csv(filename)
read.csv(filename, sep=";")
read.csv(filename, sep=",", nrows=5, row.names=1)
"В качестве названия строк взяли первый столбец, причем первый столбец id исчез.
Обратите внимание, что повторяющиеся значения в качестве наименования строк брать нельзя (например, столбец 'b')"
read.csv(filename, sep=",", nrows=5, header=FALSE)

# Работа с Excel файлами
'Пакет "readxl" упрощает импорт Excel-данных в R. Этот пакет создан для работы с табличными данными'
library(readxl)
install.packages("httr")
library(httr)
# Скачаем файл "Financial Sample.xlsx"
filename <- "Financial Sample.xlsx"
url <- "https://go.microsoft.com/fwlink/?LinkID=521962"
GET(url, write_disk(filename, overwrite=TRUE))

head(readxl::read_excel(filename))

# Импорт данных, взятых из интернет-ресурса и их обработка
install.packages("rvest")
library(rvest)
url1 <- paste0("https://cdas.cancer.gov/datasets/nlst/")
url1
page <- read_html(url1)
page
tabl <- page %>% html_nodes("table")
tabl
tabl <- tabl[[1]] %>% html_table
head(tabl)
tabl <- tabl %>% setNames(c("data", "description"))
head(tabl, 3)
#Рассмотрим отдельный столбец датафрейма
head(tabl["description"])
library(stringr)
desc_new <- stringr::str_replace(tabl$description, "\\d*.", "")
head(desc_new)

desc_new <- gsub("\n", "", desc_new)
desc_new <- trimws(desc_new)
head(desc_new)
# Сохраним обработанные данные в наш датафрейм
tabl["description"] = desc_new
dplyr::tibble(tabl)
'В R, оператор :: используется для обращения к функции или объекту, который находится внутри определенного пакета.
В данном случае, dplyr::tibble(tabl) означает, что мы используем функцию tibble из пакета dplyr,
а не из какого-то другого пакета или базового R.
Такой синтаксис с :: полезен, когда в вашем окружении уже загружено несколько пакетов,
и вы хотите явно указать, из какого пакета нужно взять определенную функцию, чтобы избежать конфликтов имен.'
# End

'В языке программирования R, термин "таблица" (tibble) обычно используется для обозначения структуры данных,
представляющей собой улучшенную версию обычного фрейма данных (data frame).
Tibble является частью пакета tibble, предоставляющего удобный и современный способ работы с данными в R.
Tibble поддерживает некоторые отличия от обычных фреймов данных в R, что делает их более удобными для работы в интерактивной среде.
Например, при выводе на экран, tibble показывает только несколько строк и столбцов, что улучшает читаемость,
и они чаще сохраняют типы данных и предоставляют более интуитивные результаты для операций.'
# Установить и загрузить пакет tibble
install.packages("tibble")
library(tibble)
# Создать тиббл
my_tibble <- tibble(
  name = c("John", "Jane", "Bob"),
  age = c(25, 30, 22),
  city = c("New York", "London", "Paris")
)
# Вывести тиббл на экран
print(my_tibble)

## Приобретенные навыки
'
1. Научились писать условие if else
2. Познакомились с полезными и простыми функциями ifelse(), sapply(), tapply(), lapply()
3. Научились писать свою собственную функцию и использовать цикл for
4. Рассмотрели метод Монте-Карло
5. Познакомились с пакетом dplyr и его полезными функциями для подготовки данных
6. Научились сохранять и импортировать данные в R
7. На реальном примере импортировали данные в R из интернет-ресурса
8. Познакомились с инструментарием для работы с текстовыми данными
'#End

##### Урок 3. Разведочный анализ данных в R
'График имеет наибольшую ценность
тогда, когда он вынуждает нас заметить
то, что мы совсем не ожидали увидеть.
Джон Тьюки'

'R уже в себе имеет мощный инструментарий для визуализации
1. Гистограммы - hist()
2. Скаттерплот - plot()
3. Боксплоты - boxplot()
4. Квантиль-квантиль плот - qqnorm(), qqline()'

library("dplyr")
library("ggplot2")
library(httr)
# Скачаем файл "cardio_train.csv"
filename <- "cardio_train.csv"
url <- "https://drive.google.com/uc?authuser=0&id=1qPKIRO3GfGNQK7rUFpZQWQRaNhTjHvJP&export=download"
GET(url, write_disk(filename, overwrite=TRUE))
df = read.csv(filename, sep=";")
head(df, 3)
dim(df)
df <- df %>% mutate(age_years=trunc(age/365))
'Это выражение берет переменную возраста, делит ее на 365, чтобы преобразовать дни в годы, а затем усекает результат.
Усечение означает удаление десятичной части без округления.
Цель состоит в том, чтобы получить целое число лет, игнорируя дробную часть.'
head(df, 3)
'Мы знаем, что СВ, которые описывают биологические процесссы, часто являются непрерывными и
следуют нормальному распределению (вес, рост, давление, скорость)
Будем исследовать 2 переменные из реального набора с kaggle.com нижние и верхнее артериальное
давление пациентов
Мы можем получить быстрое представление о этих переменных, построив для каждой из них
гистограмму с помощью функции hist()
Чтобы понять, с какими данными мы имеем дело, возьмем такие характеристики из
описательной статистики, как среднее арифметическое и среднее квадратичное
отклонение'
cat(mean(df$ap_hi), mean(df$ap_lo))
'Этот код печатает средние (средние) значения столбцов ap_hi и ap_lo из фрейма данных df.
Функцияmean() используется для расчета среднего значения.'
cat(sd(df$ap_hi), sd(df$ap_lo))
'выведет стандартное отклонение ap_hi, за которым следует стандартное отклонение ap_lo.
Стандартное отклонение — это мера величины вариации или дисперсии набора значений.'
install.packages("rafalib")
library(rafalib)
mypar(1,2)
box_lo <- boxplot(df$ap_lo)
box_hi <- boxplot(df$ap_hi)
## Что делать с выбросами?
'
* выбросы могут сорвать дальнейший анализ
* выбросы могут отображать нестабильность процесса
* выбросы могут быть ошибкой ввода
'
'Давление, опасное для жизни
 Верхние границы: 200/140
 Нижние границы: 70/50'
ap_lo_capt <- "Нижнее давление"
ap_hi_capt <- "Верхнее давление"
# Построим боксплоты без учета выбросов, которые скорее всего были ошибкой ввода
mypar(1, 2)
box_lo <- boxplot(df$ap_lo[df$ap_lo<200 & df$ap_lo>20])
title(ap_lo_capt)
box_hi <- boxplot(df$ap_hi[df$ap_hi<300 & df$ap_hi>40])
title(ap_hi_capt)

median(df$ap_lo)
quantile(df$ap_lo, 0.25)
sort(df$ap_lo)[17501:35000]
'Воспользуемся функцией filter(), %>% из пакета dplyr, чтобы подготовить датасет без
 грубейших ошибок ввода. Присвоим новому набору имя tidy_set с помощью <-'
tidy_set <- df %>% filter((ap_lo<200 & ap_lo>20) & (ap_hi<300 & ap_hi>40))
head(tidy_set, 3)

## Сравним размер прежнего набора с размером нового набора с обработанными данными
'
Посмотрим значения среднего арифметического и sd для двух переменных ap_lo и ap_hi
'
dim(tidy_set)
cat(mean(tidy_set$ap_hi), mean(tidy_set$ap_lo))
cat(sd(tidy_set$ap_hi), sd(tidy_set$ap_lo))
'После того, как мы проанализировали данные с помощью боксплота, давайте еще раз взглянем на
 гистограммы для двух величин.
 Визуализация данных с помощью боксплота дала
 нам возможность увидеть все значения переменной и определить грубейшие выбросы (аутлайеры).
 Теперь по гистограмме видно, что распределения стали больше'
mypar(2, 2)
hist(df$ap_lo, main=ap_lo_capt)
hist(tidy_set$ap_lo, main=paste(ap_lo_capt, "tidy_set"))
hist(df$ap_hi, main=ap_hi_capt)
hist(tidy_set$ap_hi, main=paste(ap_hi_capt, "tidy_set"))

## Одним из самых распространенных методов проверки нормальности является QQ график
'
В основе лежит идея сравнить теоретические квантили с квантилями СВ
'
qqnorm(tidy_set$ap_lo, main=ap_lo_capt)
qqline(tidy_set$ap_lo, col="red", lwd=2)
qqnorm(tidy_set$ap_hi, main=ap_hi_capt)
qqline(tidy_set$ap_hi, col="red", lwd=2)
abline(h=160, col="green")
'По графикам видно, что верхние значения нижнего артериального давления лежат выше, чем предполагалось нормальным распределением, а нижние - ниже
 Для верхнего артериального давления верхние значения лежат слишком высоко
 Если мы хотим сравнить давление у мужчин и женщин, удобно построить боксплоты для каждой подгруппы, используя функцию split()'
mypar(1,2)
groupss_lo <- split(tidy_set$ap_lo, tidy_set$gender)
boxplot(groupss_lo)
title(ap_lo_capt)
groupss_hi <- split(tidy_set$ap_hi, tidy_set$gender)
boxplot(groupss_hi)
title(ap_hi_capt)
'Вывод: мы видим распределение в каждой
 подгруппе и для мужчин и женщин они
 схожи. Можем предположить,что давление
 от пола не зависит'

## Вернемся к исследованию СВ без учета пола пациента
'
Построим график плотности распределения для нижнего и верхнего артериального давления
использую функцию density() и plot()
'
plot(density(tidy_set$ap_lo), col=2, lwd=2, main=ap_lo_capt)
plot(density(tidy_set$ap_hi), col=3, lwd=2, main=ap_hi_capt)
'Мы ожидали увидеть нормальное распределение с одним пиком.'
## График показал нам то, что мы не ожидали увидеть
'
Разберемся в причине этих множественных пиков
'
plot(density(tidy_set$ap_lo), col=1, lwd=2, main=ap_lo_capt)
abline(v=70, col="red")
abline(v=60, col="red")
abline(v=80, col="red")
sort_lo <- sort(tidy_set$ap_lo)
cut_1 <- sort_lo[sort_lo>65 & sort_lo<75]
cut_1
'Вывод: пики образуются за счет округления показателей давления или
отсутствия электронных приборов, которые дают большую точность измерения,
чем механические
Приблизительно так должно выглядеть истинное
нормальное распределение давления. Чтобы это изобразить, используем аргумент adjust'
plot(density(tidy_set$ap_lo, adjust=10), col=1, lwd=2, main=ap_lo_capt)
'Чтобы произвести наглядное сравнение двух случайных величин, удобно их нанести на
один график. Они будут иметь единый общий масштаб и шкалы
## Сравним распределение нижнего и верхнего артериального давления
С помощью функции legend() обозначим, каким СВ соответствуют графики'
plot(density(tidy_set$ap_lo, adjust=10), col=1, lwd=2, main=ap_lo_capt)
lines(density(tidy_set$ap_hi, adjust=10), col=3, lwd=2, lty=2)
legend("topright", c("ap_lo", "ap_hi"), col=c(2, 3), lty=c(1, 2))

## Скаттерплот
plot(
  tidy_set$ap_lo,
  tidy_set$ap_hi,
  col=2,
  main=paste(
    "correlation=",
    signif(cor(tidy_set$ap_lo, tidy_set$ap_hi), 2)
  )
)
'На предыдущих графиках мы видим
средние значения, медианы, дисперсии, но
не можем проследить взаимосвязь двух величин Скаттерплот позволяет нам это увидеть.
Скаттерплот также позволяет проследить, есть ли разделение на группы по каким-либо признакам
По данному графику можно проследить тренд: с ростом нижнего артериального давления растет и нижнее артериальное давление'
plot(
  tidy_set$ap_hi,
  tidy_set$ap_lo,
  pch=21,
  bg=as.numeric(factor(tidy_set$gender)),
  xlab=ap_hi_capt,
  ylab=ap_lo_capt
)
legend(
  "topright",
  levels(factor(tidy_set$gender)),
  col=seq(along=levels(factor(tidy_set$gender))),
  pch=15,
  cex=1.5
)
plot(
  tidy_set$weight,
  tidy_set$height,
  pch=21,
  bg=as.numeric(factor(tidy_set$gender)),
  xlab="Вес",
  ylab="Рост"
)
legend(
  "topright",
  levels(factor(tidy_set$gender)),
  col=seq(along=levels(factor(tidy_set$gender))),
  pch=15,
  cex=1.5
)
'Вывод: показатели артериального давления не смогут объяснить половую
 принадлежность пациента, в то время, как рост и вес хорошо разделены по полу'
## Скаттерплот также можно построить и для датафрейма
'
Это позволит определить, какие признаки несут больше всего информации
'
mini_set <- tidy_set[,3:5]
head(mini_set)
plot(mini_set, pch = 21, bg = mini_set$gender)

mini_set_1 <- tidy_set[,c(3,6,7)]
head(mini_set_1)
plot(mini_set_1, pch=21, bg=mini_set_1$gender)
'В конечном итоге графический анализ должен быть
информативен и понятен целевой аудитории'

## Приобретенные навыки:
'
1. Научились делать визуализацию данных:
  * гистограмма,
* боксплот,
* скаттер-плот,
* график плотности распределения,
* QQ-график
2. Научились считывать информацию, представленную в графическом виде
'
# End

##### Урок 4-5. Статистический анализ в R

'Статистика дает возможность:
оценивать неизвестные параметры генеральной совокупности
оценивать эффекты: сравнивать параметры распределений
Для оценки параметров пользуются доверительным интервалом
Для оценки эффекта используют тестирование гипотезы'
# Доверительный интервал
'Z-критерий для построения доверительного интервала:
1. Критерий Z является более предпочтительным в том случае,
если известно стандартное отклонение генеральной
совокупности
2. Имеется достаточно большой объем выборки
3. Данные хорошо приближены к нормальному распределению'
install.packages("rafalib")
install.packages("effsize")
install.packages("BSDA")
install.packages("pwr")
library(dplyr)
library(rafalib)
library(effsize)
library(BSDA)
library(pwr)
'Задача: С помощью 95% доверительного интервала оценить среднее арифметическое нормально распределенной генеральной совокупности,
зная что ее стандартное отклонение равно 3, а объем выборки равен 50'
set.seed(4 )
samp <- rnorm(50,7,3)
samp
'1) Определить статистик для оценки интервала

ЦПТ хорошо работает от 30 и выше, известно стандартное
отклонение генеральной совокупности и данные хорошо
приближены к нормальному распределению, измерения
независимы, поэтому используем критерий Z'
qqnorm(samp)
qqline(samp)
Z <- qnorm(0.975) # 95% CI
Z
SE <- 3/sqrt(50)
SE
lolv <- mean(samp) - 2*SE
uplv <- mean(samp) + 2*SE
CI <- c(lolv,uplv)
CI
'Генерируя выборку, мы представляли, будто не знаем среднее арифметическое.
Теперь мы определили доверительный интервал и можем себя проверить'
7 >= CI[1] & 7 <= CI[2]
# t- критерий для построения доверительного интервала
'1. Неизвестно стандартное отклонение генеральной
совокупности
2. Выборка небольшого объема
3. Соблюдается условие нормальности и независимости
наблюдений для первых двух пунктов
Идея 95 % интервала состоит в том, что 95 % интервалов должны захватывать истинное среднее арифметическое генеральной совокупности
Сравним как работает ЦПТ на выборках разных объемов'
bigpar(1,3)
set.seed(3)

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=10")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(10,7,3)
  SE <- sd(sam)/sqrt(10)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=50")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(50,7,3)
  SE <- sd(sam)/sqrt(50)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=100")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(100,7,3)
  SE <- sd(sam)/sqrt(100)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}
'Оставим теперь тот же set.seed(3), но теперь для выборки объемом 10 будем использовать распределение Стьюдента
Если объем выборки меньше 15, требуется, чтобы данные были приближены к нормальному распределению.
Проверим это требование с помощью qq-графика'
set.seed(3)

sam <- rnorm(10,7,3)
mypar(1,1)
qqnorm(sam, col=1, lwd=2)
qqline(sam, col="red")
'Сравним интервалы для t и z критериев при небольших выборках'
bigpar(1,2)
set.seed(3)
t <- qt(0.975, 9)

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=10, t-критерий")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(10,7,3)
  SE <- sd(sam)/sqrt(10)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=10, z-критерий")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(10,7,3)
  SE <- sd(sam)/sqrt(10)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}
Z
t
c(mean(sam)-Z*SE, mean(sam)+Z*SE)
c(mean(sam)-t*SE, mean(sam)+t*SE)

# Вернемся к реальному набору данных Cardiovascular Disease
library(httr)
# Скачаем файл "cardio_train.csv"
filename <- "cardio_train.csv"
url <- "https://drive.google.com/uc?authuser=0&id=1qPKIRO3GfGNQK7rUFpZQWQRaNhTjHvJP&export=download"
GET(url, write_disk(filename, overwrite=TRUE))

df <- read.csv(filename, sep=";")
head(df, 3)

tidy_set <- df %>% filter((ap_lo<200 & ap_lo > 20) & (ap_hi<300 & ap_hi>40))
head(tidy_set)
'Перед нами стоит задача оценить среднее диастолическое давление мужчин и женщин с помощью 95% доверительного интервала'
head(tidy_set[tidy_set$ap_hi < tidy_set$ap_lo,])

tidy_set_ <- tidy_set[tidy_set$ap_hi > tidy_set$ap_lo,]
dim(tidy_set_)
dim(tidy_set)
women_ <- tidy_set_$ap_lo[tidy_set_$gender==1]
men_ <- tidy_set_$ap_lo[tidy_set_$gender==2]

# Убедимся в предположении о нормальности
'Исходя из того, что сигма неизвестна и данные
приближены к нормальному распределению (хотя
верхние и нижние значения лежат дальше, чем
предполагалось нормальным распределением),
можем использовать t-критерий.'
mypar(1,2)
qqnorm(men_, main="мужчины")
qqline(men_)
qqnorm(women_, main="женщины")
qqline(women_)
'С помощью функции summarize(), можем построить сводную таблицу, где будут подсчитаны нужные статистические значения'
tidy_set_ %>% group_by(gender) %>% summarise(
  mu=mean(ap_lo),
  k=qt(0.975, length(ap_lo)-1),
  se=sd(ap_lo)/sqrt(length(ap_lo)),
  lowlevel=mean(ap_lo)-k*se,
  hilevel=mean(ap_lo)+k*se
)

infer <- tidy_set_ %>% group_by(gender) %>% summarise(
  mu=mean(ap_lo),
  k=qt(0.975, length(ap_lo)-1),
  se=sd(ap_lo)/sqrt(length(ap_lo)),
  lowlevel=mean(ap_lo) - k*se,
  hilevel=mean(ap_lo) + k*se
)
infer

ci_w <- c(infer[1, 5], infer[1,6])
ci_w <- as.numeric(c(infer[1,5], infer[-1,6]))
ci_w

ci_m <- c(infer[2,5], infer[2,6])
ci_m <- as.numeric(ci_m)
ci_m

# Изобразим графически интервальные оценки для мужчин и женщин
plot(
  mean(women_), col=2, lwd=2, xlim=c(0.5,2.5), ylim=c(78, 84),
  ylab="среднее диастолическое давление", main="интервальная оценка"
)
interval <- c(80.75, 80.92)
lines(x=c(1,1), y=interval, col="red", lwd=3)
points(1.5,mean(men_), col=3, lwd=2)
interval_1 <- c(82.05, 82.29)
lines(x=c(1.5,1.5), y=interval_1, col="blue", lwd=3)
legend("topleft", c("women","men"), fill=c("red","blue"))

## Тестирование гипотезы
'
* одновыборочный t-критерий
* двухвыборочный t-критерий для независимых выборок
* двухвыборочный t-критерий для зависимых выборок
'
'Вероятность ошибки первого рода $\alpha$ (0.1, 0.05, 0.01)
 Вероятность ошибка второго рода $\beta$: считается, что эта вероятность не должна превышать 20%
 Мощность теста ($1-\beta$): не менее 80%'

# Что влияет на мощность теста:
'
* величины эффекта (разница между средними значениями)
* объем выборки
* выбор уровня значимости альфа
* разброс
* количество групп
'
'Мы выбираем поставщика. Поставщик заявляет ,что он изготавливает детали размером 9 см и
стандартным отклонением 0.3 см. Мы взяли 20 деталей, измерили их и получили выборку "post".
Проверить односторонним тестом, что истинное среднее не равно 9 см. (для простоты понимания
и расчета сначала проведем односторонний тест, хотя правильно - провести двусторонний)'
post <- c(
  8.812, 9.055, 8.749, 9.479, 9.099, 8.754, 9.146, 9.221, 9.173, 8.908,
  9.454, 9.117, 8.814, 8.336, 9.337, 8.987, 8.995, 9.283, 9.246, 9.178
)
### 1) Убеждаемся, что наблюдения независимы
### 2) Если небольшой объем выборки, проверяем на нормальность данные с помощью qq-графика
qqnorm(post)
qqline(post, col="red")

### 3) Установим гипотезу
'
* H0 : mu = mu0
* H1 : mu > mu0
'
### 4) Рассчитаем мощность теста:
'
Предположим, что минимальная разница между измеренным средним и средним,
заявленным производителем, которое мы хотим выявить при тестировании гипотезы 0.3 см.
Известно стандартное отклонение 0.3 и альфа 0.05.
'
# Произведем расчет мощности теста
# Размер эффекта, значения ниже которого, мы считаем, не имеют для нас смысла
# ES (effect size) = 0.3 см
# Рассчитываем Z.0 для альфа = 0.05
qnorm(0.95)

# Вычисляем значение среднего, соответствующего Z.0 = 1.645
sig <- signif(0.3 / sqrt(20), 2)
sig

sig_1 <- 9.0 + 1.645 * sig
sig_1

# Вычислим значение Z.1 (для Н1)
z_1 <- (sig_1 - 9.3) / sig
z_1

# Вычисляем мощность теста
1 - pnorm(z_1)
'
В этой задаче было бы правильнее проводить двусторонний тест, т.е.
**H1: Mu != Mu.0**
При этом нужно учитывать, что у одностороннего теста мощность больше, чем у двустороннего
Для тестирования гипотезы воспользуемся функцией z.test() из пакета "BSDA".
Выбираем критерий Z, поскольку он более предпочтителен при неизвестном стандартном отклонении
'
### 5.1) Для начала сделаем односторонний тест
z.test(post, alternative="g", mu=9, sigma.x=0.3)
'Рассчитаем наблюдаемое вручную и сравним со значением слева, что предоставляет функция'
(mean(post)-9)*sqrt(20)/3

### 5.2) Поведем тест гипотезы в R, только теперь **двусторонний**
'Сравним значения p-value для одностороннего и двустороннего теста:'
z.test(post, alternative="g", mu=9, sigma.x=0.3)

### 6) Сделаем вывод:
'
6.1) Для одностороннего теста:
    Гипотеза H0 верна (среднее арифметическое = 9 см) на уровне значимости 0.05
p-value = 0.1971

6.2) Для двустороннего теста:
    Гипотеза Н0 верна на уровне значимости 0.05
p-value = 0.3942 
'
'Другой способ сообщить результаты теста – это сообщить доверительный интервал,
который также посчитала функция z.test()'

## Для тестирования гипотезы с небольшими выборками(< 30) используют t-test,
## а также если неизвестно стандартное отклонение
### T-test:
'
1) Одновыборочный - тест Стьюдента
2) Двухвыборочный:
  2.1) Выборки с одинаковой дисперсией – тест Стьюдента
  2.2) Выборки с разными дисперсиями – тест Уэлча
'
t.test(sample(men_, 20), sample(women_, 20), var.equal = TRUE)
t.test(sample(men_, 20), sample(women_, 20), var.equal = FALSE)

### t-test для независимых выборок

'Сравним верхнее и нижнее диастолическое давление между мужчинами и женщинами (women_, men_)
1) Посмотрим на данные: достаточно большой объем выборок, независимые измерения, независимые выборки. Стандартное отклонение неизвестно. С помощью qq-графика проверяем на нормальность. Все в порядке. Следовательно, однозначно используем t-критерий. Предполагаем, что выборки с разной дисперсией.
2) Формулируем нулевую и альтернативную гипотезы:
Н0: mu = mu0
H1: mu != mu0
3) Устанавливаем уровень значимости $\alpha$ = 0.05
4) Рассчитываем мощность теста $(1-\beta)$
'
### 4.1) Для начала рассчитаем статистику d Коэна (Cohen's d)
"
Статистика Cohen's d - один из способов оценить размер эффекта.
Показывает сколько стандартных общих отклонений между средними двух групп
"
s_pool <- sqrt(
  ((length(women_)-1)*var(women_)+(length(men_)-1)*var(men_)) / 
    (length(women_)+length(men_)-2)
)
d <- (mean(men_)-mean(women_))/s_pool
d
d <- cohen.d(d=men_, women_)
d
d <- d$estimate
d
'Видим, что значение, полученное с помощью функции cohen.d(), совпадает с
вычисленным вручную'

### 4.2) Поскольку выборки разного размера, для расчета мощности используем функцию pwr.t2n.test() из пакета "pwr"
'
В противном случае можем воспользоваться pwr.t.test().
'
pwr.t2n.test(n1=length(women_), n2=length(men_), d=d, sig.level=0.05, alternative="two.sided")

### 5) С помощью функции t.test, протестируем гипотезу 

t.test(men_, women_, alternative="two.sided")

### 6) Вывод
'
Мы получили очень маленькое значение p-value и нам следует отвергнуть нулевую гипотезу.
**Но с увеличением выборки p-value будет уменьшаться**
И очень маленькие значения p-value не представляют уже научного интереса.
Очень большие выборки позволяют обнаружить очень слабые различия, которые не несут научного смысла.
Чтобы было ясно, что мы нашли было бы правильно сообщить размер эффекта в % и его доверительный интервал,
рассчитанные следующим образом:'
((mean(men_)-mean(women_)) / mean(women_)) * 100  # размер эффекта в %

t <- t.test(men_, women_, alternative="two.sided")
ci <- t$conf.int
ci / mean(women_) * 100  # доверительный интервал для ES
attr(ci, "conf.level")
"Предположим мы хотим обнаружить сильный эффект. Cohen's d = 0.8.
 Посчитаем, сколько нужно выборок для обнаружения сильного эффекта"

pwr.t2n.test(n1=20, power=0.8, d=0.8, sig.level=0.05, alternative="two.sided")
# не обнаружили сильного эффекта
t.test(sample(men_, 20), sample(women_, 35))
''
'Одновыборочный t.test'
t.test(sample(men_, 20))
'Двухвыборочный t.test с зависимыми выборками'
t.test(
  sample(men_, 20), sample(women_, 20), 
  alternative="two.sided", paired=TRUE
)

## Приобретенные навыки:
"
1. Научились находить квантили и строить доверительный интервал для оценки среднего
2. Поняли, как работает центральная предельная теорема на выборках разных объемов и когда стоит применять t-распределение
3. Научились рассчитывать мощность теста при теcтировании гипотез с известной и неизвестной сигмой
4. Познакомились со статистикой Cohen's d
5. Научились рассчитывать объем выборки для обеспечения нужной мощности теста
6. Научились проводить тест гипотезы с известной и неизвестной сигмой в R
    * Одновыборочный, двухвыборочный с зависимыми и независимыми выборками
    * Узнали, когда применяется тест Стьюдента, когда тест Уэлча
7. Научились правильно оформлять полученные результаты
    * Научились рассчитывать размер эффекта и его доверительный интервал
    * Поняли, как важно для формирования правильного вывода понимать, что с увеличением выборки, уменьшается p-value
"
#End

## Урок 4-5. Статистический анализ в R

'Статистика дает возможность:
- оценивать неизвестные параметры генеральной совокупности
- оценивать эффекты: сравнивать параметры распределений
Для оценки параметров пользуются доверительным
интервалом.

Для оценки эффекта используют тестирование гипотезы'
'## Доверительный интервал
Z-критерий для построения доверительного интервала:
1. Критерий Z является более предпочтительным в том случае,
если известно стандартное отклонение генеральной
совокупности
2. Имеется достаточно большой объем выборки
3. Данные хорошо приближены к нормальному распределению'
install.packages("rafalib")
install.packages("effsize")
install.packages("BSDA")
install.packages("pwr")
library(dplyr)
library(rafalib)
library(effsize)
library(BSDA)
library(pwr)

'Задача:
С помощью 95% доверительного интервала оценить среднее арифметическое нормально распределенной генеральной совокупности,
зная что ее стандартное отклонение равно 3, а объем выборки равен 50'
set.seed(4 ) # устанавливает стартовое значение для генератора случайных чисел.
samp <- rnorm(50,7,3) #создает вектор samp, содержащий 50 случ чисел, сгенер из норм распред.
                      # Функция rnorm принимает три аргумента:
                      # количество чисел для генерации (50),
                      #среднее значение распределения (7)
                      #и стандартное отклонение (3).
samp #выводит вектор samp, который содержит 50 случ чисел из норм распред с параметрами среднего 7 и стандартного отклонения 3.

'1) Определить статистику для оценки интервала
ЦПТ хорошо работает от 30 и выше, известно стандартное
отклонение генеральной совокупности и данные хорошо
приближены к нормальному распределению, измерения
независимы, поэтому используем критерий Z'
qqnorm(samp)
qqline(samp)

Z <- qnorm(0.975) # 95% CI
Z
SE <- 3/sqrt(50)
SE
lolv <- mean(samp) - 2*SE
uplv <- mean(samp) + 2*SE
CI <- c(lolv,uplv)
CI
'Генерируя выборку, мы представляли, будто не знаем среднее арифметическое.
Теперь мы определили доверительный интервал и можем себя проверить'
7 >= CI[1] & 7 <= CI[2]


## t- критерий для построения доверительного интервала
'1. Неизвестно стандартное отклонение генеральной
совокупности
2. Выборка небольшого объема
3. Соблюдается условие нормальности и независимости
наблюдений для первых двух пунктов'
'Идея 95 % интервала состоит в том, что 95 % интервалов должны захватывать истинное среднее арифметическое генеральной совокупности
Сравним как работает ЦПТ на выборках разных объемов'
bigpar(1,3)
set.seed(3)

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=10")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(10,7,3)
  SE <- sd(sam)/sqrt(10)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=50")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(50,7,3)
  SE <- sd(sam)/sqrt(50)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=100")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(100,7,3)
  SE <- sd(sam)/sqrt(100)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}
'Оставим теперь тот же set.seed(3), но теперь для выборки объемом 10 будем использовать распределение Стьюдента'
'Если объем выборки меньше 15, требуется, чтобы данные были приближены к нормальному распределению.
Проверим это требование с помощью qq-графика'
set.seed(3)

sam <- rnorm(10,7,3)
mypar(1,1)
qqnorm(sam, col=1, lwd=2)
qqline(sam, col="red")
'Сравним интервалы для t и z критериев при небольших выборках'
bigpar(1,2)
set.seed(3)
t <- qt(0.975, 9)

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=10, t-критерий")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(10,7,3)
  SE <- sd(sam)/sqrt(10)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}

plot(7 + c(-4,4), c(1,1), type="n", xlab="mu", ylab="доверительные интервалы", ylim=c(1,100), main="N=10, z-критерий")
abline(v=7, col="brown", lwd=2)
for (i in 1:100) {
  sam <- rnorm(10,7,3)
  SE <- sd(sam)/sqrt(10)
  CI <- c(mean(sam)-Z*SE, mean(sam)+2*SE)
  catch <- 7 >= CI[1] & 7 <= CI[2]
  color <- ifelse(catch, "blue", "red")
  lines(CI, c(i,i), col=color, lwd=2)
}
Z
t
c(mean(sam)-Z*SE, mean(sam)+Z*SE)
c(mean(sam)-t*SE, mean(sam)+t*SE)

## Вернемся к реальному набору данных Cardiovascular Disease
library(dplyr)
library(httr)
# Скачаем файл "cardio_train.csv"
filename <- "cardio_train.csv"
url <- "https://drive.google.com/uc?authuser=0&id=1qPKIRO3GfGNQK7rUFpZQWQRaNhTjHvJP&export=download"
GET(url, write_disk(filename, overwrite=TRUE))

df <- read.csv(filename, sep=";")
head(df, 3)

tidy_set <- df %>% filter((ap_lo<200 & ap_lo > 20) & (ap_hi<300 & ap_hi>40))
head(tidy_set)
'Перед нами стоит задача оценить среднее диастолическое давление мужчин и женщин с
помощью 95% доверительного интервала'
head(tidy_set[tidy_set$ap_hi < tidy_set$ap_lo,])

tidy_set_ <- tidy_set[tidy_set$ap_hi > tidy_set$ap_lo,]
dim(tidy_set_)
dim(tidy_set)
women_ <- tidy_set_$ap_lo[tidy_set_$gender==1]
men_ <- tidy_set_$ap_lo[tidy_set_$gender==2]

## Убедимся в предположении о нормальности
'Исходя из того, что сигма неизвестна и данные
приближены к нормальному распределению
(хотя верхние и нижние значения лежат дальше, чем предполагалось нормальным распределением),
можем использовать t-критерий.'
mypar(1,2)
qqnorm(men_, main="мужчины")
qqline(men_)
qqnorm(women_, main="женщины")
qqline(women_)
'С помощью функции summarize(), можем построить сводную таблицу, где будут подсчитаны нужные статистические значения'
tidy_set_ %>% group_by(gender) %>% summarise(
  mu=mean(ap_lo),
  k=qt(0.975, length(ap_lo)-1),
  se=sd(ap_lo)/sqrt(length(ap_lo)),
  lowlevel=mean(ap_lo)-k*se,
  hilevel=mean(ap_lo)+k*se
)
infer <- tidy_set_ %>% group_by(gender) %>% summarise(
  mu=mean(ap_lo),
  k=qt(0.975, length(ap_lo)-1),
  se=sd(ap_lo)/sqrt(length(ap_lo)),
  lowlevel=mean(ap_lo) - k*se,
  hilevel=mean(ap_lo) + k*se
)
infer
ci_w <- c(infer[1, 5], infer[1,6])
ci_w <- as.numeric(c(infer[1,5], infer[-1,6]))
ci_w
ci_m <- c(infer[2,5], infer[2,6])
ci_m <- as.numeric(ci_m)
ci_m
## Изобразим графически интервальные оценки для мужчин и женщин
plot(
  mean(women_), col=2, lwd=2, xlim=c(0.5,2.5), ylim=c(78, 84),
  ylab="среднее диастолическое давление", main="интервальная оценка"
)
interval <- c(80.75, 80.92)
lines(x=c(1,1), y=interval, col="red", lwd=3)
points(1.5,mean(men_), col=3, lwd=2)
interval_1 <- c(82.05, 82.29)
lines(x=c(1.5,1.5), y=interval_1, col="blue", lwd=3)
legend("topleft", c("women","men"), fill=c("red","blue"))
## Тестирование гипотезы
'
* одновыборочный t-критерий
* двухвыборочный t-критерий для независимых выборок
* двухвыборочный t-критерий для зависимых выборок
''
Вероятность ошибки первого рода $\alpha$ (0.1, 0.05, 0.01)
Вероятность ошибка второго рода $\beta$: считается, что эта вероятность не должна превышать 20%
Мощность теста ($1-\beta$): не менее 80%
'
## Что влияет на мощность теста:
'
* величины эффекта (разница между средними значениями)
* объем выборки
* выбор уровня значимости альфа
* разброс
* количество групп
''
Мы выбираем поставщика. Поставщик заявляет ,что он изготавливает детали размером 9 см и
стандартным отклонением 0.3 см. Мы взяли 20 деталей, измерили их и получили выборку "post".
Проверить односторонним тестом, что истинное среднее не равно 9 см. (для простоты понимания
и расчета сначала проведем односторонний тест, хотя правильно - провести двусторонний)
'
post <- c(
  8.812, 9.055, 8.749, 9.479, 9.099, 8.754, 9.146, 9.221, 9.173, 8.908,
  9.454, 9.117, 8.814, 8.336, 9.337, 8.987, 8.995, 9.283, 9.246, 9.178
)
### 1) Убеждаемся, что наблюдения независимы
### 2) Если небольшой объем выборки, проверяем на нормальность данные с помощью qq-графика
qqnorm(post)
qqline(post, col="red")
### 3) Установим гипотезу
'
* H0 : mu = mu0
* H1 : mu > mu0
'
### 4) Рассчитаем мощность теста:
'
Предположим, что минимальная разница между измеренным средним и средним,
заявленным производителем, которое мы хотим выявить при тестировании гипотезы 0.3 см. Известно стандартное отклонение 0.3 и альфа 0.05.
'
# Произведем расчет мощности теста
# Размер эффекта, значения ниже которого, мы считаем, не имеют для нас смысла
# ES (effect size) = 0.3 см
# Рассчитываем Z.0 для альфа = 0.05
qnorm(0.95)
# Вычисляем значение среднего, соответствующего Z.0 = 1.645
sig <- signif(0.3 / sqrt(20), 2)
sig
sig_1 <- 9.0 + 1.645 * sig
sig_1
# Вычислим значение Z.1 (для Н1)
z_1 <- (sig_1 - 9.3) / sig
z_1
# Вычисляем мощность теста
1 - pnorm(z_1)
'
В этой задаче было бы правильнее проводить двусторонний тест, т.е.
**H1: Mu != Mu.0**
При этом нужно учитывать, что у одностороннего теста мощность больше, чем у двустороннего
Для тестирования гипотезы воспользуемся функцией z.test() из пакета "BSDA".
Выбираем критерий Z, поскольку он более предпочтителен при неизвестном стандартном отклонении.
'
### 5.1) Для начала сделаем односторонний тест
install.packages("BSDA")
library(BSDA)
z.test(post, alternative="g", mu=9, sigma.x=0.3)
'Рассчитаем наблюдаемое вручную и сравним со значением слева, что предоставляет функция'
(mean(post)-9)*sqrt(20)/3
### 5.2) Поведем тест гипотезы в R, только теперь **двусторонний**
'Сравним значения p-value для одностороннего и двустороннего теста:'
z.test(post, alternative="g", mu=9, sigma.x=0.3)
z.test(post, alternative="two.sided", mu=9, sigma.x=0.3)
### 6) Сделаем вывод:
'
6.1) Для одностороннего теста:
Гипотеза H0 верна (среднее арифметическое = 9 см) на уровне значимости 0.05
p-value = 0.1971
6.2) Для двустороннего теста:
Гипотеза Н0 верна на уровне значимости 0.05
p-value = 0.3942 
'
'Другой способ сообщить результаты теста – это сообщить доверительный интервал,
который также посчитала функция z.test()'
## Для тестирования гипотезы с небольшими выборками(< 30) используют t-test, а также если неизвестно стандартное отклонение
### T-test:
'
1) Одновыборочный - тест Стьюдента
2) Двухвыборочный:
2.1) Выборки с одинаковой дисперсией – тест Стьюдента
2.2) Выборки с разными дисперсиями – тест Уэлча
'
t.test(sample(men_, 20), sample(women_, 20), var.equal = TRUE)
t.test(sample(men_, 20), sample(women_, 20), var.equal = FALSE)

### t-test для независимых выборок
'Сравним верхнее и нижнее диастолическое давление между мужчинами и женщинами (women_, men_)
1) Посмотрим на данные: достаточно большой объем выборок, независимые измерения, независимые выборки. Стандартное отклонение неизвестно. С помощью qq-графика проверяем на нормальность. Все в порядке. Следовательно, однозначно используем t-критерий. Предполагаем, что выборки с разной дисперсией.
2) Формулируем нулевую и альтернативную гипотезы:
Н0: mu = mu0
H1: mu != mu0
3) Устанавливаем уровень значимости $\alpha$ = 0.05
4) Рассчитываем мощность теста $(1-\beta)$

### 4.1) Для начала рассчитаем статистику d Коэна (Cohen's d)
"
Статистика Cohen's d - один из способов оценить размер эффекта.
Показывает сколько стандартных общих отклонений между средними двух групп
"
library(effsize)
s_pool <- sqrt(
  ((length(women_)-1)*var(women_)+(length(men_)-1)*var(men_)) / 
    (length(women_)+length(men_)-2)
)
d <- (mean(men_)-mean(women_))/s_pool
d
d <- cohen.d(d=men_, women_)
d
d <- d$estimate
d
'Видим, что значение, полученное с помощью функции cohen.d(), совпадает с вычисленным вручную'

### 4.2) Поскольку выборки разного размера, для расчета мощности используем функцию pwr.t2n.test() из пакета "pwr"
'
В противном случае можем воспользоваться pwr.t.test().
'
library(pwr)
pwr.t2n.test(n1=length(women_), n2=length(men_), d=d, sig.level=0.05, alternative="two.sided")
### 5) С помощью функции t.test, протестируем гипотезу 
t.test(men_, women_, alternative="two.sided")


### 6) Вывод:
'
Мы получили очень маленькое значение p-value и нам следует отвергнуть нулевую гипотезу. **Но с увеличением выборки p-value будет уменьшаться**
И очень маленькие значения p-value не представляют уже научного интереса. Очень большие выборки позволяют обнаружить очень слабые различия, которые не несут научного смысла.
Чтобы было ясно, что мы нашли было бы правильно сообщить размер эффекта в % и его доверительный интервал, рассчитанные следующим образом:
'
((mean(men_)-mean(women_)) / mean(women_)) * 100  # размер эффекта в %
t <- t.test(men_, women_, alternative="two.sided")
ci <- t$conf.int
ci / mean(women_) * 100  # доверительный интервал для ES
attr(ci, "conf.level")
"Предположим мы хотим обнаружить сильный эффект. Cohen's d = 0.8.
Посчитаем, сколько нужно выборок для обнаружения сильного эффекта"
pwr.t2n.test(n1=20, power=0.8, d=0.8, sig.level=0.05, alternative="two.sided")
# не обнаружили сильного эффекта
t.test(sample(men_, 20), sample(women_, 35))
#В данном случае H0 верна

'Одновыборочный t.test'
t.test(sample(men_, 20))

'Двухвыборочный t.test с зависимыми выборками'
t.test(
  sample(men_, 20), sample(women_, 20), 
  alternative="two.sided", paired=TRUE
)

## Приобретенные навыки:
"
1. Научились находить квантили и строить доверительный интервал для оценки среднего.
2. Поняли, как работает центральная предельная теорема на выборках разных объемов и когда стоит применять t-распределение.
3. Научились рассчитывать мощность теста при теcтировании гипотез с известной и неизвестной сигмой.
4. Познакомились со статистикой Cohen's d.
5. Научились рассчитывать объем выборки для обеспечения нужной мощности теста.
6. Научились проводить тест гипотезы с известной и неизвестной сигмой в R.
    * Одновыборочный, двухвыборочный с зависимыми и независимыми выборками.
    * Узнали, когда применяется тест Стьюдента, когда тест Уэлча.
7. Научились правильно оформлять полученные результаты.
    * Научились рассчитывать размер эффекта и его доверительный интервал.
    * Поняли, как важно для формирования правильного вывода понимать, что с увеличением выборки, уменьшается p-value.
"
# END

## Урок 6. Статистический анализ в R. Anova. Регрессионный анализ

'Математическая модель - это описание поведения
какой-либо реальной случайной величины на языке
математики'

## Линейная регрессия
'В основе лежит предположение некой ЛИНЕЙНОЙ зависимости $Y \sim X$,где 
$Y$ – зависимая переменная,
$X$ - независимая(-ые)
$Y= f(X)$
Для парной линейной регрессии, т.е. где только одна независимая
переменная (признак) $Х$, линейная зависимость будет иметь вид:
$y = \beta_0 + \beta_1 * X$
С помощью линейной регрессии попробуем описать зависимость переменной $y$ (верхнее давление)
от переменной $Х$, в качестве которой мы будем рассматривать различные колонки и их комбинации из подготовленного набора данных tidy_set'

install.packages("rafalib")
install.packages("effsize")
install.packages("BSDA")
install.packages("pwr")
library(httr)
library(dplyr)
library(rafalib)
library(effsize)
library(BSDA)
library(pwr)

# Скачаем файл "cardio_train.csv"
filename <- "cardio_train.csv"
url <- "https://drive.google.com/uc?authuser=0&id=1qPKIRO3GfGNQK7rUFpZQWQRaNhTjHvJP&export=download"
GET(url, write_disk(filename, overwrite=TRUE))

df <- read.csv(filename, sep=";")
head(df, 3)

df$age_years <- trunc(df$age / 365.25)

tidy_set <- df %>% filter((ap_lo<200 & ap_lo>20) & (ap_hi<300 & ap_hi>40))
head(tidy_set)

tidy_set <- tidy_set[tidy_set$ap_hi > tidy_set$ap_lo,]
head(tidy_set)

dim(tidy_set)

set.seed(1)
ind <- sample(seq(1, nrow(tidy_set)), 100)
ind

ts <- tidy_set[ind,]
head(ts)

dim(ts)

plot(ts$ap_lo, ts$ap_hi, xlim=c(0,120), ylim=c(0,200), col="red", lwd=2)

'Из графика видно, что прослеживается линейная зависимость
между нижним и верхним давлением пациента'

## Функции lm() и predict() 

'Построим парную линейную регрессию, в качестве независимой переменной возьмем нижнее давление пациента'

fitm <- lm(ts$ap_hi ~ ts$ap_lo)
fitm

hi_hat <- 32.961 + 1.121 * ts$ap_lo
hi_hat

'Функция predict() упрощает вычисления оценочного параметра,
ее особенно удобно использовать,когда имеем дело не с одним Х
'
as.numeric(predict(fitm, ts))

'Причина различий оценочных значений $y$ кроется в округлении вычислений'
signif(fitm$coefficients, 10)

32.96074004 + 1.121065203*ts$ap_lo

plot(seq(1:length(ts$ap_hi)), ts$ap_hi, col="red", type="l")
lines(seq(1, length(hi_hat)), hi_hat, col="blue", type="l")
legend("topleft", c("ts$ap_hi", "hi_hat"), col=c(2,3), lty=c(1,1))
summary(fitm)
'
1. Формула
2. Residuals: Распределение остатков
3. Coefficients: Значение коэффициентов для подобранной модели, значимость коэффициентов модели (t-критерий)
4. Residual standard error
5. Multiple R-squared коэффициент детерминации
6. F-statistic: Значимость модели в целом (F-критерий)
'
### Residuals:
'
Одним из важных условий для построения модели
линейной регрессии является предположение, что
ошибки следуют нормальному распределению
'
qqnorm(fitm$residuals)
qqline(fitm$residuals)

### Coefficients:
'
С помощью t-статистики Стьюдента можно проверить значимость коэффициентов построенной модели
'
summary(fitm)

'R дает значения t-статистики, p-value и с помощью «*» отмечает наиболее значимые коэффициенты.
В строке Signif.codes приведена расшифровка обозначений:
например, «***» соответствуют p-value, лежащей между нулем и 0.001'

### Residual Standard Error:
'
RSE вычисляется следующим образом
'
rse <- sqrt(sum(residuals(fitm)^2) / fitm$df.residual)
rse
'98 степеней свободы = k-n-1, где k - объем выборки,
n - число предикторов (в нашем случае 1 предиктор - это нижнее давление)
'
### R-squared:
'
Коэффициент детерминации. Показывает, какую часть изменчивости величины y описала построенная модель.
Посчитать эту величину можно следующим образом:
'
Rs <- cor(ts$ap_hi, ts$ap_lo)^2
Rs
'С увеличением числа предикторов коэффициент детерминации растет'

### Adjusted R-squared:
'
Решает эту проблему. Если добавленные новые предикторы не
вносят весомого вклада в модель, то этот параметр будет падать, в противном случае - расти
'
R_adj <- 1-((1-Rs)*((100-1)/(100-1-1)))
R_adj

### F-statistic:
'
Позволяет оценить значимость построенной модели в целом
'
tsn <- ts[,-c(1,2)]
head(tsn)

## ANOVA
'
Дисперсионный анализ (ANOVA - analysis of variance) используется,
когда мы хотим выяснить влияние одного или нескольких факторов
(качественных переменных) на количественную переменную (отклик).

Используем, чтобы избежать множественных
сравнений, которые приводят к росту вероятности
ошибки первого рода.

При использовании поправок на множественные
сравнения, растет вероятность ошибки второго рода с
ужесточением уровня значимости.
'
'Задача сводится к сравнению средних арифметических по подгруппам.

В ANOVA мы имеем дело с:
- объясненной (факторной дисперсией, межгрупповой)
- необъясненной (внутригрупповой)

На данном рисунке видно, что межгрупповая
дисперсия очень мала, что предполагает, что
данный фактор не оказывает влияния на числовую переменную. С помощью ANOVA мы можем проверить
наше предположение'

plot(
  tidy_set$gluc, tidy_set$ap_hi, cex=1, col=tidy_set$gluc,
  xlab="уровень глюкозы", ylab="верхнее давление", xlim=c(0.7, 3.5)
)
## Три важных условия при проведении дисперсионного анализа:
'
1. Случайность и независимость измерений - **ОБЯЗАТЕЛЬНОЕ**
2. Переменная-отклик следует нормальному распределению в группах
3. Гомоскедастичность дисперсий

При невыполнении 2 или 3 условия дисперсионного анализа растет вероятность принять значимые факторы за незначимые.

Но ко 2 условию ANOVA менее чувствителен.

Если одно из этих условий не выполняется, следует это отметить в конечном
результате исследования
'
## Сбалансированные и несбалансированные данные
'
Если данные имеют разное количество наблюдений в группе, то мы имеем дело с несбалансированными данными
'
# работаем с выборкой
set.seed(1)
ind <- sample(seq(1, nrow(tidy_set)), 100)
ind

ts <- tidy_set[ind,]
head(ts)

table(ts$gluc)

table(ts$gender, ts$gluc)

'- Если размеры выборок одинаковые, то неоднородность дисперсий слабо влияет на результат.
   Несбалансированные данные особенно важны при неоднородности дисперсий
 - Слабые отклонения от нормальности не сильно влияют на результат.
   В однофакторном дисперсионном анализе при больших объемах выборок можно пренебречь этим условием

Если данные имеют дисбаланс, то ANOVA становится более чувствительным к нарушениям условий его применения.
Нарушение условий ведет к росту вероятности ошибки первого рода'

### ВЫВОД: стараемся делать одинаковые выборки 

## Задача: проведем исследование влияния 2-х факторов: пол и уровень глюкозы на верхнее давление
### 1. Берем выборки. Соблюдаем условие случайности и независимости

head(tidy_set)

set.seed(1)
# g1 - gluc == 1; s1 - gender == 1
# g2 - gluc == 2; s2 - gender == 2
g1_s1 <- sample(tidy_set$ap_hi[tidy_set$gluc==1 & tidy_set$gender==1], 20)
g1_s1
g1_s2 <- sample(tidy_set$ap_hi[tidy_set$gluc==1 & tidy_set$gender==2], 20)
g1_s2

g2_s1 <- sample(tidy_set$ap_hi[tidy_set$gluc==2 & tidy_set$gender==1], 20)
g2_s1
g2_s2 <- sample(tidy_set$ap_hi[tidy_set$gluc==2 & tidy_set$gender==2], 20)
g2_s2

g3_s1 <- sample(tidy_set$ap_hi[tidy_set$gluc==3 & tidy_set$gender==1], 20)
g3_s1
g3_s2 <- sample(tidy_set$ap_hi[tidy_set$gluc==3 & tidy_set$gender==2], 20)
g3_s2

### На этом этапе цель: построить датафрейм

# новый вектор "gender_new" и "gluc_new"
gender_new <- c(rep(1,20), rep(2,20), rep(1,20), rep(2,20), rep(1,20), rep(2,20))
gender_new

gluc_new <- c(rep(1,40), rep(2,40), rep(3,40))
gluc_new

sam_s <-c(g1_s1, g1_s2, g2_s1, g2_s2, g3_s1, g3_s2)
sam_s

anovaframe <- data.frame(sam_s, gender_new, gluc_new)  # соблюдаются случайность и независимость
head(anovaframe, 25)

table(anovaframe$gender_new, anovaframe$gluc_new)

### 2) Разведочный анализ
### 2.1) Для наглядности представим данные графически

boxplot(
  sam_s~gender_new, data=anovaframe, boxwex=0.15, at=1:2-0.3,
  subset=gluc_new==1, col="green", main="EDA ANOVA",
  xlab="пол", ylab="верхнее давление",
  xlim=c(0.5, 2.5), ylim=c(0, 200)
)
boxplot(
  sam_s~gender_new, data=anovaframe, add=TRUE,
  boxwex=0.2, at=1:2-0.1, subset=gluc_new==2, col="orange"
)
boxplot(
  sam_s~gender_new, data=anovaframe, add=TRUE,
  boxwex=0.2, at=1:2+0.15, subset=gluc_new==3, col="brown"
)
legend(
  "bottomleft",
  c("gluc=1", "gluc=2", "gluc=3"),
  fill=c("green", "orange", "brown")
)
'Видим небольшую неоднородность дисперсий'

### 2.2. Помимо визуальной оценки однородности дисперсий (п. 2.1) проверим гомоскедастичность с помощью специальных критериев
'
Распространенные критерии:
'
'
Критерий           | Функция          | Условия применения
-------------------|------------------|------------------
F- критерий	var.test()	1) Для сравнения 2-х дисперсий

2) Возможен разный объем выборок
Критерий Бартлетта	bartlett.test()	1) Для множественных сравнений

2) Объемы выборки могут быть различны, но не менее 3

3) Должно соблюдаться условие нормальности.

Тест очень чувствительный к нарушению этого условия
Критерий Кохрена	cochran.test()

package "outliers"	1) Для множественных авнений

2) Одинаковый объем выборок
Критерий Левенэ	leveneTest()

package "car"	Аналог критерий Балетта, но читается менее чувствительным

к нарушению условия нормальности
'
### Воспользуемся критерием Бартлетта
'
Условие нормальности соблюдается хорошо
'
bartlett.test(list(g1_s1, g1_s2, g2_s1, g2_s2, g3_s1, g3_s2))

'Принимаем нулевую гипотезу на уровне значимости 0.05.
Статистически значимых различий между дисперсиями выборок нет.
Все условия соблюдены.
Теперь можно приступать непосредственно к самому дисперсионному анализу'

### 2.3) Проверим предположение о нормальности распределений с помощью qq-графика

mypar(2,3)
qqnorm(g1_s1)
qqline(g1_s1)
qqnorm(g1_s2)
qqline(g1_s2)
qqnorm(g2_s1)
qqline(g2_s1)
qqnorm(g2_s2)
qqline(g2_s2)
qqnorm(g3_s1)
qqline(g3_s1)
qqnorm(g3_s2)
qqline(g3_s2)

'Есть совсем небольшие отклонения. Нас это устаивает.
 Тем более, что мы имеем одинаковые объемы выборок
 Сбалансированные данные НЕ влияют на порядок включения факторов в модель'

summary(aov(sam_s~gender_new + gluc_new + gender_new:gluc_new, data=anovaframe))

summary(aov(sam_s~gluc_new + gender_new + gluc_new:gender_new, data=anovaframe))
'Чтобы не прописывать эффект взаимодействия, факторы в модели указывают с помощью знака «*»'
summary(aov(sam_s~gender_new * gluc_new, data=anovaframe))


### Интерпретация результата

'Взаимодействие факторов «уровень глюкозы» и «пол», а также сами факторы
не оказывают значимого эффекта на давление пациента на уровне значимости 0.05.'

## Приобретенные навыки:
'
1. Научились подбирать линейную модель с помощью функции lm()
2. Интерпретировать результат: оценивать значимость коэффициентов (критерий t) и самой модели в целом (критерий F)
3. Рассмотрели чем отличаются параметры $R$ и $R_{adj}$
4. Рассмотрели условия для проведения ANOVA
5. Составили общую схему действий для ANOVA при сбалансированных и несбалансированных данных
6. Изучили различные методы в R проверки выборок на однородность дисперсий
7. Интерпретировали конечный результат статистического анализа
'
## END






























































































































